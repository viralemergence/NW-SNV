{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, List, Optional, Sequence, Tuple, Union\n",
    "import re\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FastaAnnotator:\n",
    "    \"\"\"\n",
    "    Reusable annotator for aligned FASTA files that:\n",
    "      1) Normalizes headers (uppercase; canonical accession tokens).\n",
    "      2) Splits sequences into two groups:\n",
    "         - \"NCBI side\" (non-Whitman): merged to a TSV of NCBI metadata by accession.\n",
    "         - \"Whitman side\" (SNVT-tagged): matched to Excel metadata by a canonical sample ID.\n",
    "      3) Normalizes and attaches dates (keeps sequences even if date missing).\n",
    "      4) Composes final FASTA headers with a user-defined template and writes output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fasta_aligned_path : str\n",
    "        Path to the aligned FASTA.\n",
    "    ncbi_metadata_path : str\n",
    "        Path to a TSV with an 'accession' column (case-insensitive), plus any other fields.\n",
    "    submitter_col : str\n",
    "        Column name in the NCBI TSV used for state mapping (substring match).\n",
    "    state_map : Dict[str, str]\n",
    "        Mapping from submitter name substrings to USA 2-letter state codes (e.g., {\"HJELLE\":\"NM\"}).\n",
    "    whitman_sources : Sequence[Tuple[str, str, str, str]]\n",
    "        Sequence of (excel_path, sheet_name, sample_col, date_col) for Whitman metadata inputs.\n",
    "    whitman_us_state : str\n",
    "        Two-letter state code to assign to Whitman sequences (default: \"WA\").\n",
    "    header_template : str\n",
    "        Template for final FASTA header. Available fields: {sample}, {usa}, {collection_date}.\n",
    "    sequence_colname : str\n",
    "        Column name for sequences in the internal DataFrame.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    • Whitman detection is case-insensitive for the token 'SNVT' anywhere in the header.\n",
    "    • Canonical NCBI accession token is derived from the first header token:\n",
    "        - Take header → UPPERCASE\n",
    "        - Split on space or pipe → take first segment\n",
    "        - Strip trailing version suffix: .<digits>\n",
    "    • Whitman sample IDs are extracted from headers matching:\n",
    "        ..._SNVT_<PREFIX>_<NUM>[_FILLED]  →  PREFIX<NUM>, with PEMA→PESO normalization.\n",
    "      Fallbacks also support WHIT#, EP#, or last-token heuristics.\n",
    "    • Whitman metadata uses the same sample-extraction logic as the FASTA, uppercased.\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs\n",
    "    fasta_aligned_path: str\n",
    "    ncbi_metadata_path: str\n",
    "    submitter_col: str = \"submitters\"\n",
    "    state_map: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "    # Whitman metadata sources as (file_path, sheet_name, sample_col, date_col)\n",
    "    whitman_sources: Sequence[Tuple[str, str, str, str]] = field(default_factory=list)\n",
    "\n",
    "    # Configuration\n",
    "    whitman_us_state: str = \"WA\"\n",
    "    header_template: str = \"{sample}|{usa}|{collection_date}\"\n",
    "    sequence_colname: str = \"sequence\"\n",
    "\n",
    "    # Internals (populated after annotate())\n",
    "    aln_dict: Dict[str, SeqRecord] = field(init=False, default_factory=dict)  # keys: FULL UPPER headers; rec.name: accession_token\n",
    "    ncbi_df: pd.DataFrame = field(init=False, default_factory=pd.DataFrame)\n",
    "    merged_ncbi_df: pd.DataFrame = field(init=False, default_factory=pd.DataFrame)\n",
    "    whitman_df: pd.DataFrame = field(init=False, default_factory=pd.DataFrame)\n",
    "    combined_df: pd.DataFrame = field(init=False, default_factory=pd.DataFrame)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Public API\n",
    "    # ----------------------------\n",
    "\n",
    "    def annotate(self) -> pd.DataFrame:\n",
    "        \"\"\"Run the full pipeline and return the combined annotated DataFrame.\"\"\"\n",
    "        self._load_alignment()\n",
    "        self._load_ncbi_metadata()\n",
    "\n",
    "        left = self._build_ncbi_side()\n",
    "        right = self._build_whitman_side()\n",
    "\n",
    "        combined = pd.concat([left, right], ignore_index=True, sort=False)\n",
    "\n",
    "        # Normalize / render dates and compose final headers\n",
    "        combined[\"collection_date\"] = combined[\"collection_date\"].apply(self._normalize_collection_date)\n",
    "        combined = self._compose_headers(combined)\n",
    "\n",
    "        self.combined_df = combined\n",
    "        return combined\n",
    "\n",
    "    def write_fasta(self, output_path: str) -> None:\n",
    "        \"\"\"Write annotated sequences to FASTA using composed 'header' as the SeqRecord.id.\"\"\"\n",
    "        if self.combined_df.empty:\n",
    "            raise RuntimeError(\"No data to write. Run annotate() first.\")\n",
    "        records = [\n",
    "            SeqRecord(\n",
    "                Seq(seq),\n",
    "                id=hdr,\n",
    "                description=\"\"\n",
    "            )\n",
    "            for hdr, seq in zip(self.combined_df[\"header\"], self.combined_df[self.sequence_colname])\n",
    "        ]\n",
    "        with open(output_path, \"w\") as handle:\n",
    "            SeqIO.write(records, handle, \"fasta\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Stage 0: I/O + Normalization\n",
    "    # ----------------------------\n",
    "\n",
    "    def _load_alignment(self) -> None:\n",
    "        \"\"\"\n",
    "        Load aligned FASTA into a dict of SeqRecords with:\n",
    "          • KEY  : full UPPERCASE header (unchanged content, just uppercased)\n",
    "          • rec.name : canonical accession token for NCBI merges\n",
    "        \"\"\"\n",
    "        raw = SeqIO.to_dict(SeqIO.parse(self.fasta_aligned_path, \"fasta\"))\n",
    "        cleaned: Dict[str, SeqRecord] = {}\n",
    "\n",
    "        for k, rec in raw.items():\n",
    "            kU = str(k).upper()  # global case normalization\n",
    "            # Canonical accession token for NCBI:\n",
    "            #   - take first segment before space or pipe\n",
    "            #   - strip trailing .<digits> version suffix\n",
    "            first = kU.split(\" \", 1)[0].split(\"|\", 1)[0]\n",
    "            token = re.sub(r\"\\.\\d+$\", \"\", first)  # e.g., OQ999106.1 → OQ999106\n",
    "\n",
    "            rec_clean = SeqRecord(rec.seq, id=kU, name=token, description=\"\")\n",
    "            cleaned[kU] = rec_clean\n",
    "\n",
    "        self.aln_dict = cleaned\n",
    "\n",
    "    def _load_ncbi_metadata(self) -> None:\n",
    "        \"\"\"Load NCBI metadata TSV and standardize column names + accession case.\"\"\"\n",
    "        df = pd.read_csv(self.ncbi_metadata_path, sep=\"\\t\")\n",
    "        df.columns = df.columns.str.lower()\n",
    "        if \"accession\" not in df.columns:\n",
    "            raise ValueError(\"NCBI metadata must contain an 'accession' column.\")\n",
    "        df[\"accession\"] = df[\"accession\"].astype(str).str.upper()\n",
    "        self.ncbi_df = df\n",
    "\n",
    "    # ----------------------------\n",
    "    # Stage 1: Frame builders\n",
    "    # ----------------------------\n",
    "\n",
    "    def _df_from_alignment(self, keep_if) -> pd.DataFrame:\n",
    "        \"\"\"Create a DataFrame with 'header', 'accession_token', and 'sequence' for records passing keep_if(header).\"\"\"\n",
    "        rows: List[Dict[str, str]] = []\n",
    "        for key, rec in self.aln_dict.items():\n",
    "            if keep_if(key):\n",
    "                rows.append({\n",
    "                    \"header\": key,                          # full UPPER header (for Whitman parsing)\n",
    "                    \"accession_token\": rec.name,            # canonical accession (for NCBI merges)\n",
    "                    self.sequence_colname: str(rec.seq)\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def _build_ncbi_side(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Non-Whitman sequences: merge with NCBI metadata by canonical accession token.\n",
    "        Adds 'usa' via submitter mapping (if provided).\n",
    "        Produces columns: ['accession', 'sequence', 'usa', 'collection_date', 'sample'].\n",
    "        \"\"\"\n",
    "        aln_ncbi = self._df_from_alignment(lambda k: not self._is_whitman(k))\n",
    "\n",
    "        # Prepare for merge: NCBI metadata 'accession' must match 'accession_token'\n",
    "        merged = pd.merge(self.ncbi_df, aln_ncbi,\n",
    "                          left_on=\"accession\", right_on=\"accession_token\", how=\"inner\")\n",
    "\n",
    "        # Assign USA state via submitter mapping (if any)\n",
    "        merged = self._map_submitters_to_state(merged, self.submitter_col, self.state_map)\n",
    "\n",
    "        # Normalize/ensure required columns exist\n",
    "        out = merged.copy()\n",
    "        if \"collection_date\" in out.columns:\n",
    "            out[\"collection_date\"] = out[\"collection_date\"].apply(self._normalize_collection_date)\n",
    "        else:\n",
    "            out[\"collection_date\"] = pd.NaT\n",
    "\n",
    "        # For the NCBI side, we default 'sample' to the accession token unless a 'sample' column exists\n",
    "        out[\"sample\"] = out.get(\"sample\", out[\"accession\"])\n",
    "\n",
    "        if \"usa\" not in out.columns:\n",
    "            out[\"usa\"] = pd.NA\n",
    "\n",
    "        out.rename(columns={self.sequence_colname: \"sequence\"}, inplace=True)\n",
    "        out = out[[\"accession\", \"sequence\", \"usa\", \"collection_date\", \"sample\"]].copy()\n",
    "        return out\n",
    "\n",
    "    def _build_whitman_side(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Whitman sequences (headers containing 'SNVT', case-insensitive):\n",
    "          • Derive 'sample' from the header via SNVT regex (PEMA→PESO at sample level).\n",
    "          • LEFT join to Whitman metadata (do NOT drop rows on missing dates).\n",
    "          • Assign usa = whitman_us_state.\n",
    "        Produces columns: ['accession', 'sequence', 'usa', 'collection_date', 'sample'].\n",
    "        \"\"\"\n",
    "        aln_whit = self._df_from_alignment(self._is_whitman)\n",
    "        if aln_whit.empty:\n",
    "            return pd.DataFrame(columns=[\"accession\", \"sequence\", \"usa\", \"collection_date\", \"sample\"])\n",
    "\n",
    "        # Derive sample (from full header) and USA\n",
    "        whit = aln_whit.copy()\n",
    "        whit[\"sample\"] = whit[\"header\"].apply(self._sample_extractor)\n",
    "        whit[\"usa\"] = self.whitman_us_state\n",
    "\n",
    "        # Load Whitman metadata, normalized to same sample keys\n",
    "        meta_whit = self._load_whitman_metadata()\n",
    "\n",
    "        # LEFT join: keep sequences even if metadata/date missing\n",
    "        whit = pd.merge(whit, meta_whit, on=\"sample\", how=\"left\")\n",
    "\n",
    "        # Compose output schema\n",
    "        whit.rename(columns={\"accession_token\": \"accession\", self.sequence_colname: \"sequence\"}, inplace=True)\n",
    "        out = whit[[\"accession\", \"sequence\", \"usa\", \"collection_date\", \"sample\"]].copy()\n",
    "        return out\n",
    "\n",
    "    # ----------------------------\n",
    "    # Stage 2: Metadata loaders and utilities\n",
    "    # ----------------------------\n",
    "\n",
    "    def _load_whitman_metadata(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load and harmonize Whitman metadata to ['sample', 'collection_date'].\n",
    "        • Uppercase sample strings, then apply SAME extractor as for FASTA headers.\n",
    "        • Keep rows even if collection_date is missing (no dropping; left-join later).\n",
    "        \"\"\"\n",
    "        frames: List[pd.DataFrame] = []\n",
    "        for (path, sheet, sample_col, date_col) in self.whitman_sources:\n",
    "            tmp = pd.read_excel(path, sheet_name=sheet)[[sample_col, date_col]].copy()\n",
    "            tmp.columns = [\"sample_raw\", \"collection_date_raw\"]\n",
    "            frames.append(tmp)\n",
    "\n",
    "        if frames:\n",
    "            meta = pd.concat(frames, ignore_index=True)\n",
    "        else:\n",
    "            meta = pd.DataFrame(columns=[\"sample_raw\", \"collection_date_raw\"])\n",
    "\n",
    "        # Normalize to uppercase and derive canonical sample\n",
    "        meta[\"sample\"] = meta[\"sample_raw\"].astype(str).str.upper().apply(self._sample_extractor)\n",
    "\n",
    "        # Parse dates; allow missing\n",
    "        meta[\"collection_date\"] = meta[\"collection_date_raw\"].apply(self._normalize_collection_date)\n",
    "\n",
    "        return meta[[\"sample\", \"collection_date\"]].copy()\n",
    "\n",
    "    @staticmethod\n",
    "    def _map_submitters_to_state(df: pd.DataFrame,\n",
    "                                 submitter_col: str,\n",
    "                                 state_map: Dict[str, str]) -> pd.DataFrame:\n",
    "        \"\"\"Assign two-letter state codes based on substring matches in submitter_col.\"\"\"\n",
    "        if not state_map or submitter_col not in df.columns:\n",
    "            return df\n",
    "        if \"usa\" not in df.columns:\n",
    "            df[\"usa\"] = pd.NA\n",
    "\n",
    "        # Case-insensitive substring match\n",
    "        sub = df[submitter_col].astype(str).str.upper()\n",
    "        for name, state in state_map.items():\n",
    "            mask = sub.str.contains(str(name).upper(), na=False)\n",
    "            df.loc[mask, \"usa\"] = state\n",
    "        return df\n",
    "\n",
    "    # ----------------------------\n",
    "    # Stage 3: Parsing / Normalization helpers\n",
    "    # ----------------------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_whitman(header: str) -> bool:\n",
    "        \"\"\"Detect Whitman entries by presence of 'SNVT' (case-insensitive).\"\"\"\n",
    "        return \"SNVT\" in str(header).upper()\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_collection_date(val: Union[str, pd.Timestamp, None]) -> Optional[pd.Timestamp]:\n",
    "        \"\"\"\n",
    "        Normalize collection dates:\n",
    "          - 'YYYY'        -> YYYY-06-01\n",
    "          - 'YYYY-MM'     -> YYYY-MM-01\n",
    "          - 'YYYY-MM-DD'  -> as-is\n",
    "          - otherwise: pandas-coerced (may be NaT)\n",
    "        \"\"\"\n",
    "        if val is None or (isinstance(val, float) and pd.isna(val)):\n",
    "            return pd.NaT\n",
    "        if isinstance(val, pd.Timestamp):\n",
    "            return pd.to_datetime(val.date())\n",
    "\n",
    "        s = str(val).strip()\n",
    "        if not s or s.lower() in {\"nan\", \"nat\"}:\n",
    "            return pd.NaT\n",
    "\n",
    "        if re.fullmatch(r\"\\d{4}\", s):\n",
    "            return pd.to_datetime(f\"{s}-06-01\", errors=\"coerce\")\n",
    "        if re.fullmatch(r\"\\d{4}-\\d{2}\", s):\n",
    "            return pd.to_datetime(f\"{s}-01\", errors=\"coerce\")\n",
    "        return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _safe_str(x) -> str:\n",
    "        return \"\" if pd.isna(x) else str(x)\n",
    "\n",
    "    def _compose_headers(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create final FASTA headers using the header_template.\"\"\"\n",
    "        df = df.copy()\n",
    "        # Render date to ISO string (YYYY-MM-DD or empty)\n",
    "        date_str = df[\"collection_date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        df[\"header\"] = [\n",
    "            self.header_template.format(\n",
    "                sample=self._safe_str(s),\n",
    "                usa=self._safe_str(u),\n",
    "                collection_date=self._safe_str(d)\n",
    "            )\n",
    "            for s, u, d in zip(df[\"sample\"], df[\"usa\"], date_str)\n",
    "        ]\n",
    "        return df\n",
    "\n",
    "    # ----------------------------\n",
    "    # Sample extraction logic\n",
    "    # ----------------------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample_extractor(header_or_sample: str) -> str:\n",
    "        \"\"\"\n",
    "        Derive a canonical sample ID from either a FASTA header or a raw metadata sample string.\n",
    "        Priority patterns (all case-insensitive; input is uppercased internally):\n",
    "          1) ..._SNVT_<PREFIX>_<NUM>[_FILLED]  →  PREFIX<NUM>  (with PEMA→PESO normalization)\n",
    "          2) WHIT<NUM>                          →  WHIT<NUM>   (no leading zeros)\n",
    "          3) EP<NUM>                            →  <NUM>\n",
    "          4) Fallback: last '_' token (alnum only)\n",
    "        \"\"\"\n",
    "        h = str(header_or_sample).upper().strip()\n",
    "\n",
    "        # Prefer SNVT pattern (allow optional trailing _FILLED)\n",
    "        m = re.search(r\"_SNVT_([A-Z]+)_(\\d+)(?:_FILLED)?$\", h)\n",
    "        if m:\n",
    "            prefix, num = m.group(1), int(m.group(2))\n",
    "            if prefix == \"PEMA\":  # normalize at the sample level (safe)\n",
    "                prefix = \"PESO\"\n",
    "            return f\"{prefix}{num}\"\n",
    "\n",
    "        # WHIT<NUM>\n",
    "        m = re.search(r\"(WHIT)0?(\\d{1,4})\", h)\n",
    "        if m:\n",
    "            return f\"WHIT{int(m.group(2))}\"\n",
    "\n",
    "        # EP<NUM> → numeric only\n",
    "        m = re.search(r\"(EP)0?(\\d+)\", h)\n",
    "        if m:\n",
    "            return m.group(2)\n",
    "\n",
    "        # Conservative fallback: last '_' token stripped to [A-Z0-9]\n",
    "        last = h.split(\"_\")[-1]\n",
    "        return re.sub(r\"[^A-Z0-9]+\", \"\", last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accession</th>\n",
       "      <th>sequence</th>\n",
       "      <th>usa</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>sample</th>\n",
       "      <th>header</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OQ999106</td>\n",
       "      <td>ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...</td>\n",
       "      <td>CA</td>\n",
       "      <td>1999-04-29</td>\n",
       "      <td>OQ999106</td>\n",
       "      <td>OQ999106|CA|1999-04-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OQ999109</td>\n",
       "      <td>ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACCACTGCAA...</td>\n",
       "      <td>CA</td>\n",
       "      <td>1999-06-21</td>\n",
       "      <td>OQ999109</td>\n",
       "      <td>OQ999109|CA|1999-06-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQ999111</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>CA</td>\n",
       "      <td>1999-04-14</td>\n",
       "      <td>OQ999111</td>\n",
       "      <td>OQ999111|CA|1999-04-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OQ999114</td>\n",
       "      <td>ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...</td>\n",
       "      <td>CA</td>\n",
       "      <td>2000-03-04</td>\n",
       "      <td>OQ999114</td>\n",
       "      <td>OQ999114|CA|2000-03-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OQ999119</td>\n",
       "      <td>ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...</td>\n",
       "      <td>CO</td>\n",
       "      <td>2011-03-28</td>\n",
       "      <td>OQ999119</td>\n",
       "      <td>OQ999119|CO|2011-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2DZNTT_6_6_SNVT_PEMA_230_FILLED</td>\n",
       "      <td>ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...</td>\n",
       "      <td>WA</td>\n",
       "      <td>NaT</td>\n",
       "      <td>PESO230</td>\n",
       "      <td>PESO230|WA|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>YMD9RR_2_SNVT_PEMA_261_FILLED</td>\n",
       "      <td>ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...</td>\n",
       "      <td>WA</td>\n",
       "      <td>NaT</td>\n",
       "      <td>PESO261</td>\n",
       "      <td>PESO261|WA|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>YMD9RR_3_SNVT_PEMA_287_FILLED</td>\n",
       "      <td>ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...</td>\n",
       "      <td>WA</td>\n",
       "      <td>NaT</td>\n",
       "      <td>PESO287</td>\n",
       "      <td>PESO287|WA|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>YMD9RR_4_SNVT_PEMA_295_FILLED</td>\n",
       "      <td>ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...</td>\n",
       "      <td>WA</td>\n",
       "      <td>NaT</td>\n",
       "      <td>PESO295</td>\n",
       "      <td>PESO295|WA|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2DZNTT_2_2_SNVT_PEMA_165</td>\n",
       "      <td>ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...</td>\n",
       "      <td>WA</td>\n",
       "      <td>NaT</td>\n",
       "      <td>PESO165</td>\n",
       "      <td>PESO165|WA|</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accession  \\\n",
       "0                          OQ999106   \n",
       "1                          OQ999109   \n",
       "2                          OQ999111   \n",
       "3                          OQ999114   \n",
       "4                          OQ999119   \n",
       "..                              ...   \n",
       "66  2DZNTT_6_6_SNVT_PEMA_230_FILLED   \n",
       "67    YMD9RR_2_SNVT_PEMA_261_FILLED   \n",
       "68    YMD9RR_3_SNVT_PEMA_287_FILLED   \n",
       "69    YMD9RR_4_SNVT_PEMA_295_FILLED   \n",
       "70         2DZNTT_2_2_SNVT_PEMA_165   \n",
       "\n",
       "                                             sequence usa collection_date  \\\n",
       "0   ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...  CA      1999-04-29   \n",
       "1   ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACCACTGCAA...  CA      1999-06-21   \n",
       "2   ----------------------------------------------...  CA      1999-04-14   \n",
       "3   ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...  CA      2000-03-04   \n",
       "4   ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...  CO      2011-03-28   \n",
       "..                                                ...  ..             ...   \n",
       "66  ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...  WA             NaT   \n",
       "67  ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...  WA             NaT   \n",
       "68  ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...  WA             NaT   \n",
       "69  ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...  WA             NaT   \n",
       "70  ATGGTAGGGTGGGTTTGCATCTTCCTCGTGGTCCTTACTACTGCAA...  WA             NaT   \n",
       "\n",
       "      sample                  header  \n",
       "0   OQ999106  OQ999106|CA|1999-04-29  \n",
       "1   OQ999109  OQ999109|CA|1999-06-21  \n",
       "2   OQ999111  OQ999111|CA|1999-04-14  \n",
       "3   OQ999114  OQ999114|CA|2000-03-04  \n",
       "4   OQ999119  OQ999119|CO|2011-03-28  \n",
       "..       ...                     ...  \n",
       "66   PESO230             PESO230|WA|  \n",
       "67   PESO261             PESO261|WA|  \n",
       "68   PESO287             PESO287|WA|  \n",
       "69   PESO295             PESO295|WA|  \n",
       "70   PESO165             PESO165|WA|  \n",
       "\n",
       "[71 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot = FastaAnnotator(\n",
    "    fasta_aligned_path=\"../Avail_seqs/M/SNV_M_NCBI_MEZAP.fasta\",\n",
    "    ncbi_metadata_path=\"../Avail_seqs/ncbi_data.tsv\",\n",
    "    submitter_col=\"submitters\",\n",
    "    state_map={\"GOODFELLOW\":\"NM\",\"HECHT\":\"AZ\",\"HJELLE\":\"NM\",\"BOTTEN\":\"NM\",\"SPIROPOULOU\":\"NM\"},\n",
    "    whitman_sources=[\n",
    "        (\"../Avail_seqs/rodent_sero_ELISAdata.xlsx\", \"RNAextractionLung\", \"Individual\", \"Date Sampled\"),\n",
    "        (\"../Avail_seqs/WHIT.Samples.EEIDP.xlsx\", \"3.3.25 - RNA Extraction\", \"Sample ID Number\", \"Collection Date\"),\n",
    "    ],\n",
    "    whitman_us_state=\"WA\",\n",
    "    header_template=\"{sample}|{usa}|{collection_date}\"\n",
    ")\n",
    "df = annot.annotate()\n",
    "annot.write_fasta(\"../Avail_seqs/ncbi_whitman_aligned.fasta\")\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
